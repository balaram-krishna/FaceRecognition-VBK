{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Downgrade numpy to avoid version conflicts\n",
        "!pip install numpy==1.25.0 --quiet\n",
        "\n",
        "# Install required packages\n",
        "!pip install facenet-pytorch==2.5.2 gradio torch torchvision opencv-python pillow --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37JelaEcwkYc",
        "outputId": "8de5150d-5568-4665-ca33-d94640352f68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "0txH_8eJxKzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import cv2\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import gradio as gr\n",
        "\n",
        "# Use GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Initialize models\n",
        "mtcnn = MTCNN(keep_all=True, device=device, margin=20, post_process=True)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Face matching function\n",
        "def match_faces(solo_img, group_img, threshold=0.65):\n",
        "    try:\n",
        "        # Convert to RGB\n",
        "        solo_img = solo_img.convert(\"RGB\")\n",
        "        group_img = group_img.convert(\"RGB\")\n",
        "        image_np = np.array(group_img)\n",
        "\n",
        "        # --- Solo face extraction (only FIRST face is used) ---\n",
        "        solo_faces = mtcnn(solo_img)\n",
        "        if solo_faces is None:\n",
        "            return \"No faces detected in solo image.\", group_img\n",
        "\n",
        "        # If multiple faces are detected in solo, pick the first one only\n",
        "        if solo_faces.ndim == 4:\n",
        "            solo_face = solo_faces[0].unsqueeze(0).to(device)\n",
        "        elif solo_faces.ndim == 3:\n",
        "            solo_face = solo_faces.unsqueeze(0).to(device)\n",
        "        else:\n",
        "            return \"Invalid solo face detection.\", group_img\n",
        "\n",
        "        solo_embedding = resnet(solo_face)\n",
        "\n",
        "        # --- Group faces detection ---\n",
        "        group_faces = mtcnn(group_img)\n",
        "        boxes, _ = mtcnn.detect(group_img)\n",
        "\n",
        "        if boxes is None or group_faces is None:\n",
        "            return \"No faces detected in group image.\", group_img\n",
        "\n",
        "        total_faces = len(boxes)\n",
        "        match_found = False\n",
        "\n",
        "        # Compare each group face strictly against the solo face\n",
        "        for i, box in enumerate(boxes):\n",
        "            group_face_tensor = group_faces[i].unsqueeze(0).to(device)\n",
        "            group_emb = resnet(group_face_tensor)\n",
        "\n",
        "            sim_score = torch.nn.functional.cosine_similarity(group_emb, solo_embedding).item()\n",
        "\n",
        "            # Only mark as a match if it passes strict threshold\n",
        "            if sim_score > threshold:\n",
        "                match_found = True\n",
        "                color = (0, 255, 0)  # Green box\n",
        "                label = f\"Match: {sim_score*100:.1f}%\"\n",
        "            else:\n",
        "                # Ignore all other detections (do not label or mark them)\n",
        "                continue\n",
        "\n",
        "            # Draw only matched face\n",
        "            x1, y1, x2, y2 = [int(v) for v in box]\n",
        "            cv2.rectangle(image_np, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(image_np, label, (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        result_text = f\"Total faces in group: {total_faces}\\nMatch found: {'Yes' if match_found else 'No'}\"\n",
        "        return result_text, Image.fromarray(image_np)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {str(e)}\", group_img\n",
        "\n",
        "\n",
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=match_faces,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Solo Image (only ONE person is used)\"),\n",
        "        gr.Image(type=\"pil\", label=\"Group Image\"),\n",
        "        gr.Slider(minimum=0.5, maximum=0.8, step=0.01, value=0.65, label=\"Match Threshold (Strict)\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Result\"),\n",
        "        gr.Image(label=\"Detected Face (only solo person)\"),\n",
        "    ],\n",
        "    title=\"Strict Face Match Detection\",\n",
        "    description=\"Detects ONLY the person in the solo image inside the group image. Ignores all other faces, even if similar.\"\n",
        ")\n",
        "\n",
        "# Launch with public link\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "WetADrGOxTVX",
        "outputId": "4cd3dafc-8949-4e5c-9730-fa5012cbdae4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://137a9cce6a830d2dfe.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://137a9cce6a830d2dfe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}